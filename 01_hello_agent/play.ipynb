{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncAzureOpenAI\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    function_tool,\n",
    "    set_default_openai_api,\n",
    "    set_default_openai_client,\n",
    "    set_tracing_disabled,\n",
    ")\n",
    "from agents.run import RunConfig\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3114124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: /home/chicha/repos/openai-sdk-101/01_hello_agent/.env\n",
      "Does .env file exist? True\n",
      "API KEY: 2geV8...\n",
      "API VERSION: 2025-01-01-preview\n",
      "ENDPOINT: https://open-aj.openai.azure.com\n",
      "DEPLOYMENT: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path to the .env file\n",
    "current_dir = Path().absolute()\n",
    "env_path = current_dir / \".env\"\n",
    "print(f\"Looking for .env file at: {env_path}\")\n",
    "print(f\"Does .env file exist? {env_path.exists()}\")\n",
    "\n",
    "# Load environment variables from .env file with explicit path\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Debug environment variables to ensure they're loaded\n",
    "print(\n",
    "    f\"API KEY: {os.getenv('AZURE_OPENAI_API_KEY', '')[:5] if os.getenv('AZURE_OPENAI_API_KEY') else 'Not found'}...\"\n",
    ")\n",
    "print(f\"API VERSION: {os.getenv('AZURE_OPENAI_API_VERSION', 'Not found')}\")\n",
    "print(f\"ENDPOINT: {os.getenv('AZURE_OPENAI_ENDPOINT', 'Not found')}\")\n",
    "print(f\"DEPLOYMENT: {os.getenv('AZURE_OPENAI_DEPLOYMENT', 'Not found')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI client created successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging to disable verbose HTTP request logs from OpenAI client\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "try:\n",
    "    azure_openai_client = AsyncAzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "    )\n",
    "    print(\"Azure OpenAI client created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Azure OpenAI client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client test successful:\n",
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Test if the client works with a simple completion\n",
    "try:\n",
    "    response = await azure_openai_client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\"),\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "    )\n",
    "    print(\"Client test successful:\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Azure OpenAI client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully\n",
      "Config created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = OpenAIChatCompletionsModel(\n",
    "        model=\"gpt-4o\", openai_client=azure_openai_client\n",
    "    )\n",
    "    print(\"Model created successfully\")\n",
    "    config = RunConfig(\n",
    "        model=model,\n",
    "        tracing_disabled=True,\n",
    "    )\n",
    "    print(\"Config created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model or config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe35389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    agent = Agent(\n",
    "        name=\"Master Agent\",\n",
    "        model=model,\n",
    "    )\n",
    "    print(\"Agent created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating agent: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f058e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CALLING AGENT\n",
      "\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How are *you* doing today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use the async version instead of run_sync to avoid event loop conflicts\n",
    "    async def run_agent():\n",
    "        result = await Runner.run(agent, \"Hello, how are you.\", run_config=config)\n",
    "        print(\"\\nCALLING AGENT\\n\")\n",
    "        print(result.final_output)\n",
    "        return result\n",
    "\n",
    "    # Execute the async function directly - Jupyter will handle it properly\n",
    "    result = await run_agent()\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "\n",
    "    print(f\"Error running agent: {e}\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66ac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words weave through my mind,  \n",
      "A guide born to help and share,  \n",
      "Whispers in the wind.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(name=\"Assistant\", instructions=\"You only respond in haikus.\")\n",
    "result = await Runner.run(agent, \"Tell me about yourself.\", run_config=config)\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e9ac8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_openai_client(client=azure_openai_client, use_for_tracing=False)\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "set_tracing_disabled(disabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11000be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the weather for a given location.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {location} is sunny with a high of 75Â°F.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074db1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in New York is currently sunny with a high of 75Â°F.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You're a helpful assistant.\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "result = await Runner.run(agent, \"What is the weather in New York?\", run_config=config)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0bcefec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions='You only respond in haikus.', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1745076683.3897088, error=None, incomplete_details=None, instructions=None, metadata=None, model='gpt-4o', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.created'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Code', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' that', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' calls', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' itself', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='  \\n', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Loops', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' within', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' loops', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' depth', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' unfolds', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=',', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='  \\n', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Rec', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='ursion', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"'s\", item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' clean', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' dance', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='__fake_id__', output_index=0, type='response.output_text.delta'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Code that calls itself,  \\nLoops within loops, depth unfolds,  \\nRecursion's clean dance.\", type='output_text'), type='response.content_part.done'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Code that calls itself,  \\nLoops within loops, depth unfolds,  \\nRecursion's clean dance.\", type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1745076683.3897088, error=None, incomplete_details=None, instructions=None, metadata=None, model='gpt-4o', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Code that calls itself,  \\nLoops within loops, depth unfolds,  \\nRecursion's clean dance.\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), type='response.completed'), type='raw_response_event')RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions='You only respond in haikus.', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Code that calls itself,  \\nLoops within loops, depth unfolds,  \\nRecursion's clean dance.\", type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    agent = Agent(name=\"Assistant\", instructions=\"You only respond in haikus.\")\n",
    "    result = Runner.run_streamed(agent, \"Tell me about recursion in programming.\", run_config=config)\n",
    "    #print(result.final_output)\n",
    "    async for message in result.stream_events():\n",
    "        print(message, end=\"\", flush=True)\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
